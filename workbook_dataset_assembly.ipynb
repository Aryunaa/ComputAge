{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "from computage.utils.data_utils import download_meta, download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GSE203399', 'GSE87648', 'GSE32148', 'GSE59685', 'GSE53840', 'GSE42861', 'GSE69138', 'GSE62867', 'GSE56581', 'GSE62003', 'GSE87640', 'GSE56046', 'GSE49909', 'GSE107143']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "datasets_needed = ['GSE56046', 'GSE56581', 'GSE62867', 'GSE69138', \n",
    "             'GSE107143', 'GSE203399', 'GSE32148', 'GSE53840',\n",
    "              'GSE53841', 'GSE67705', 'GSE67751', 'GSE77696', \n",
    "              'GSE81961', 'GSE87640', 'GSE87648', 'GSE100264', \n",
    "              'GSE107080', 'GSE117859', 'GSE117860', 'GSE140800', \n",
    "              'GSE143942', 'GSE145714', 'GSE185389', 'GSE185390', \n",
    "              'GSE217633', 'GSE49909', 'GSE56606', 'GSE62003', \n",
    "              'GSE131461', 'GSE166611', 'GSE193836', 'GSE42861', \n",
    "              'GSE71841', 'GSE99624', 'GSE131989', 'GSE134429', \n",
    "              'GSE137593', 'GSE137594', 'GSE138653', 'GSE175364', \n",
    "              'GSE176168', 'GSE228104', 'GSE43976', 'GSE59685', \n",
    "              'GSE72774', 'GSE72776', 'GSE103929', 'GSE106648', \n",
    "              'GSE111223', 'GSE111629', 'GSE112596', 'GSE122244', \n",
    "              'GSE130029', 'GSE130030', 'GSE130491', 'GSE144858', \n",
    "              'GSE151355', 'GSE156994', 'GSE219293', 'GSE72338', \n",
    "              'GSE118468', 'GSE118469', 'GSE145714', 'GSE131752', \n",
    "              'GSE182991', 'GSE214297']\n",
    "\n",
    "datasets_present = ['GSE107143', 'GSE109096', 'GSE157131', 'GSE197670',\n",
    "                     'GSE203399', 'GSE222595', 'GSE32148', 'GSE38291', \n",
    "                     'GSE42861', 'GSE46394', 'GSE48325', 'GSE49909', \n",
    "                     'GSE50498', 'GSE52588', 'GSE53840', 'GSE56046', \n",
    "                     'GSE56581', 'GSE59685', 'GSE61256', 'GSE62003', \n",
    "                     'GSE62867', 'GSE69138', 'GSE73103', 'GSE80970', \n",
    "                     'GSE84395', 'GSE87640', 'GSE87648', 'GSE132203', \n",
    "                     'GSE30870', 'GSE36054', 'GSE40279', 'GSE41169', \n",
    "                     'GSE51032', 'GSE55763', 'GSE64495', 'GSE69270', \n",
    "                     'GSE72775', 'GSE87571']\n",
    "\n",
    "datasets_load = list(set(datasets_needed)&set(datasets_present))\n",
    "print(datasets_load)\n",
    "print(len(datasets_load))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant 1. Downloading and saving table with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset GSE59685 saved to computage/temp_loaded_files\n",
      "GSE59685 dataframe df was created:  data: (531, 485577)  meta_data: (531, 10)\n",
      "GSE59685 dataframe was filtered rows: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14420/3913735079.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data_tissue['SourceGSE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computage/temp_loaded_files/GSE59685.pkl is saved\n",
      "Dataset GSE69138 saved to computage/temp_loaded_files\n",
      "GSE69138 dataframe df was created:  data: (185, 456344)  meta_data: (185, 8)\n",
      "GSE69138 dataframe was filtered rows: 0\n",
      "computage/temp_loaded_files/GSE69138.pkl is saved\n",
      "Dataset GSE107143 saved to computage/temp_loaded_files\n",
      "GSE107143 dataframe df was created:  data: (16, 460295)  meta_data: (16, 7)\n",
      "GSE107143 dataframe was filtered rows: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14420/3913735079.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data_tissue['SourceGSE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computage/temp_loaded_files/GSE107143.pkl is saved\n",
      "Dataset GSE56046 saved to computage/temp_loaded_files\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dir = \"computage/temp_loaded_files\"\n",
    "tissue = 'Blood'\n",
    "df_tissue = pd.DataFrame()\n",
    "df_tissue_meta = pd.DataFrame()\n",
    "\n",
    "for filename in datasets_load:\n",
    "    #loading\n",
    "    meta = download_meta('./meta_table_datasets.xlsx')\n",
    "    download_dataset(meta, filename, dir)\n",
    "\n",
    "    #reading -> saving\n",
    "    filepath_pklgz = dir + '/' + filename + '.pkl.gz'\n",
    "    filepath_pkl = dir + '/' + filename + '.pkl'\n",
    "    \n",
    "    with gzip.open(filepath_pklgz,'rb')as f_in:\n",
    "        with open(filepath_pkl, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "    df = pd.read_pickle(filepath_pkl)\n",
    "\n",
    "    #reading pkl\n",
    "    meta_data = df.get('meta')\n",
    "    data = df.get('data')\n",
    "    print(filename, 'dataframe df was created:', ' data:', data.shape, ' meta_data:', meta_data.shape) ##\n",
    "\n",
    "    #filtering, creating data_tissue, meta_data_tissue\n",
    "    meta_data_tissue = meta_data[(meta_data['Condition'] == 'HC')&(meta_data['Tissue'] == tissue)]\n",
    "    # meta_data_tissue.loc[:, 'SourceGSE'] = filename\n",
    "    meta_data_tissue['SourceGSE'] = filename\n",
    "    data_tissue = data.loc[data.index.isin(meta_data_tissue.index)]\n",
    "    print(filename, 'dataframe was filtered', 'rows:', data_tissue.shape[0]) ##\n",
    "\n",
    "    #saving tmp_csvs\n",
    "    data_tissue.to_csv(dir + '/temp_csv/' + filename + '.csv')\n",
    "    meta_data_tissue.to_csv(dir + '/temp_csv/' + filename +'_meta' + '.csv')\n",
    "    print(filepath_pkl, 'is saved')\n",
    "\n",
    "    #merging\n",
    "    df_tissue = pd.concat([df_tissue, data_tissue])\n",
    "    df_tissue_meta = pd.concat([df_tissue_meta, meta_data_tissue])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant 2 (step by step). Downloading and saving table with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading, saving to csv: metadata, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n",
      "FILE 11\n",
      "###############################\n",
      "Dataset GSE87640 saved to computage/temp_loaded_files\n",
      "GSE87640 dataframe df was created:  data: (240, 485577)  meta_data: (240, 10)\n",
      "GSE87640 dataframe was filtered rows: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17540/2067655150.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data_tissue['SourceGSE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computage/temp_loaded_files/GSE87640.pkl is saved\n",
      "###############################\n",
      "FILE 12\n",
      "###############################\n",
      "Dataset GSE53840 saved to computage/temp_loaded_files\n",
      "GSE53840 dataframe df was created:  data: (120, 485577)  meta_data: (120, 26)\n",
      "GSE53840 dataframe was filtered rows: 0\n",
      "computage/temp_loaded_files/GSE53840.pkl is saved\n",
      "###############################\n",
      "FILE 13\n",
      "###############################\n",
      "Dataset GSE87648 saved to computage/temp_loaded_files\n",
      "GSE87648 dataframe df was created:  data: (382, 460398)  meta_data: (382, 10)\n",
      "GSE87648 dataframe was filtered rows: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17540/2067655150.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data_tissue['SourceGSE'] = filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computage/temp_loaded_files/GSE87648.pkl is saved\n",
      "###############################\n",
      "FILE 14\n",
      "###############################\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dir = \"computage/temp_loaded_files\"\n",
    "tissue = 'Blood'\n",
    "counter = 0\n",
    "\n",
    "for filename in datasets_load:\n",
    "    print('#######')\n",
    "    print(\"FILE\", counter)\n",
    "    print('#######')\n",
    "    counter += 1\n",
    "\n",
    "    #loading\n",
    "    meta = download_meta('./meta_table_datasets.xlsx')\n",
    "    download_dataset(meta, filename, dir)\n",
    "\n",
    "    #reading -> saving\n",
    "    filepath_pklgz = dir + '/' + filename + '.pkl.gz'\n",
    "    filepath_pkl = dir + '/' + filename + '.pkl'\n",
    "    \n",
    "    with gzip.open(filepath_pklgz,'rb')as f_in:\n",
    "        with open(filepath_pkl, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "    df = pd.read_pickle(filepath_pkl)\n",
    "\n",
    "    #reading pkl\n",
    "    meta_data = df.get('meta')\n",
    "    data = df.get('data')\n",
    "    print(filename, 'dataframe df was created:', ' data:', data.shape, ' meta_data:', meta_data.shape) ##\n",
    "\n",
    "    #filtering, creating data_tissue, meta_data_tissue\n",
    "    meta_data_tissue = meta_data[(meta_data['Condition'] == 'HC')&(meta_data['Tissue'] == tissue)]\n",
    "    meta_data_tissue['SourceGSE'] = filename\n",
    "    data_tissue = data.loc[data.index.isin(meta_data_tissue.index)]\n",
    "    print(filename, 'dataframe was filtered', 'rows:', data_tissue.shape[0]) ##\n",
    "\n",
    "    #saving tmp_csvs\n",
    "    data_tissue.to_csv(dir + '/temp_csv/' + filename + '.csv')\n",
    "    meta_data_tissue.to_csv(dir + '/temp_csv/' + filename +'_meta' + '.csv')\n",
    "    print(filepath_pkl, 'is saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"computage/temp_loaded_files\"\n",
    "dir_csv = dir + '/temp_csv/'\n",
    "table_prep = pd.DataFrame()\n",
    "column_names = []\n",
    "\n",
    "for filename in datasets_load:\n",
    "\n",
    "    with open(dir_csv + filename + '_meta.csv','r') as file:\n",
    "        header = file.readline().split(sep=',')\n",
    "        column_names.extend(header)\n",
    "\n",
    "column_names = sorted(list(set(column_names)))\n",
    "\n",
    "# Создаем пустую таблицу со всеми колонками\n",
    "table_prep =  pd.DataFrame(column_names).T\n",
    "table_prep.columns = table_prep.iloc[0,:]\n",
    "table_prep = table_prep.drop(table_prep.index)\n",
    "table_prep\n",
    "\n",
    "\n",
    "full_table_tissue_name = 'full_blood_HC'\n",
    "table_prep.to_csv('computage/' + full_table_tissue_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2087/510909243.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_meta_tissue = pd.concat([table_prep, meta_data_tissue])\n",
      "/tmp/ipykernel_2087/510909243.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_meta_tissue = pd.concat([table_prep, meta_data_tissue])\n",
      "/tmp/ipykernel_2087/510909243.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_meta_tissue = pd.concat([table_prep, meta_data_tissue])\n",
      "/tmp/ipykernel_2087/510909243.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_meta_tissue = pd.concat([table_prep, meta_data_tissue])\n",
      "/tmp/ipykernel_2087/510909243.py:3: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  full_meta_tissue = pd.concat([table_prep, meta_data_tissue])\n"
     ]
    }
   ],
   "source": [
    "for filename in datasets_load:\n",
    "    meta_data_tissue = pd.read_csv(dir_csv + filename + '_meta.csv')\n",
    "    full_meta_tissue = pd.concat([table_prep, meta_data_tissue])\n",
    "    full_meta_tissue = full_meta_tissue.set_index('Unnamed: 0')\n",
    "    full_meta_tissue.to_csv('computage/' + full_table_tissue_name + '.csv', header=False, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь для данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"computage/temp_loaded_files\"\n",
    "dir_csv = dir + '/temp_csv/'\n",
    "table_prep = pd.DataFrame()\n",
    "column_names = []\n",
    "\n",
    "\n",
    "for filename in datasets_load:\n",
    "\n",
    "    with open(dir_csv + filename + '.csv','r') as file:\n",
    "        header = file.readline().split(sep=',')\n",
    "        column_names.extend(header)\n",
    "\n",
    "column_names = list(set(column_names))\n",
    "\n",
    "# Создаем пустую таблицу со всеми колонками\n",
    "table_prep =  pd.DataFrame(column_names).T\n",
    "table_prep.columns = table_prep.iloc[0,:]\n",
    "table_prep = table_prep.drop(table_prep.index)\n",
    "table_prep\n",
    "\n",
    "\n",
    "full_table_tissue_name = 'full_blood_HC_CpGs'\n",
    "table_prep.to_csv('computage/' + full_table_tissue_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487183"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_prep.columns.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m data_tissue \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dir_csv \u001b[38;5;241m+\u001b[39m filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m full_meta_tissue \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([table_prep, data_tissue])\n\u001b[0;32m----> 4\u001b[0m full_meta_tissue \u001b[38;5;241m=\u001b[39m full_meta_tissue\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m full_meta_tissue\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputage/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m full_table_tissue_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:6186\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6184\u001b[0m \u001b[38;5;66;03m# use set to handle duplicate column names gracefully in case of drop\u001b[39;00m\n\u001b[1;32m   6185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(to_remove):\n\u001b[0;32m-> 6186\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m frame[c]\n\u001b[1;32m   6188\u001b[0m \u001b[38;5;66;03m# clear up memory usage\u001b[39;00m\n\u001b[1;32m   6189\u001b[0m index\u001b[38;5;241m.\u001b[39m_cleanup()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/generic.py:4507\u001b[0m, in \u001b[0;36mNDFrame.__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m deleted:\n\u001b[1;32m   4503\u001b[0m     \u001b[38;5;66;03m# If the above loop ran and didn't delete anything because\u001b[39;00m\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;66;03m# there was no match, this call should raise the appropriate\u001b[39;00m\n\u001b[1;32m   4505\u001b[0m     \u001b[38;5;66;03m# exception:\u001b[39;00m\n\u001b[1;32m   4506\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m-> 4507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39midelete(loc)\n\u001b[1;32m   4509\u001b[0m \u001b[38;5;66;03m# delete from the caches\u001b[39;00m\n\u001b[1;32m   4510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/managers.py:1442\u001b[0m, in \u001b[0;36mBlockManager.idelete\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   1439\u001b[0m is_deleted[indexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m taker \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m~\u001b[39mis_deleted)\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1442\u001b[0m nbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(taker, only_slice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ref_inplace_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1443\u001b[0m new_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;241m~\u001b[39mis_deleted]\n\u001b[1;32m   1444\u001b[0m axes \u001b[38;5;241m=\u001b[39m [new_columns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/managers.py:761\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    756\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_fill \u001b[38;5;129;01mand\u001b[39;00m only_slice:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# GH#33597 slice instead of take, so we get\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;66;03m#  views instead of copies\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    762\u001b[0m         blk\u001b[38;5;241m.\u001b[39mgetitem_block_columns(\n\u001b[1;32m    763\u001b[0m             \u001b[38;5;28mslice\u001b[39m(ml, ml \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    764\u001b[0m             new_mgr_locs\u001b[38;5;241m=\u001b[39mBlockPlacement(i),\n\u001b[1;32m    765\u001b[0m             ref_inplace_op\u001b[38;5;241m=\u001b[39mref_inplace_op,\n\u001b[1;32m    766\u001b[0m         )\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, ml \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(slobj)\n\u001b[1;32m    768\u001b[0m     ]\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/managers.py:762\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    756\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_fill \u001b[38;5;129;01mand\u001b[39;00m only_slice:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# GH#33597 slice instead of take, so we get\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;66;03m#  views instead of copies\u001b[39;00m\n\u001b[1;32m    761\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 762\u001b[0m         blk\u001b[38;5;241m.\u001b[39mgetitem_block_columns(\n\u001b[1;32m    763\u001b[0m             \u001b[38;5;28mslice\u001b[39m(ml, ml \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    764\u001b[0m             new_mgr_locs\u001b[38;5;241m=\u001b[39mBlockPlacement(i),\n\u001b[1;32m    765\u001b[0m             ref_inplace_op\u001b[38;5;241m=\u001b[39mref_inplace_op,\n\u001b[1;32m    766\u001b[0m         )\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, ml \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(slobj)\n\u001b[1;32m    768\u001b[0m     ]\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/blocks.py:360\u001b[0m, in \u001b[0;36mBlock.getitem_block_columns\u001b[0;34m(self, slicer, new_mgr_locs, ref_inplace_op)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03mPerform __getitem__-like, return result as block.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03mOnly supports slices that preserve dimensionality.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(slicer)\n\u001b[0;32m--> 360\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_inplace_op \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs\u001b[38;5;241m.\u001b[39mhas_reference() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(new_values, new_mgr_locs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for filename in datasets_load:\n",
    "    data_tissue = pd.read_csv(dir_csv + filename + '.csv')\n",
    "    full_meta_tissue = pd.concat([table_prep, data_tissue])\n",
    "    full_meta_tissue = full_meta_tissue.set_index('Unnamed: 0')\n",
    "    full_meta_tissue.to_csv('computage/' + full_table_tissue_name + '.csv', header=False, mode='a')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
